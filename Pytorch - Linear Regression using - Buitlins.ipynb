{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch.nn package from pytorch contains utility classes for building neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input (temp,rainfall,humidity)\n",
    "\n",
    "inputs=np.array([[73,67,43],[91,88,64],[87,134,58],[102,43,37],[69,96,70],[73,67,43],\n",
    "                [91,88,64],[87,134,58],[102,43,37],[69,96,70],[73,67,43],[91,88,64],\n",
    "                [87,134,58],[102,43,37],[69,96,70]],dtype=\"float32\")\n",
    "\n",
    "# Targets (apples and oranges)\n",
    "targets= np.array([[56,70],[81,101],[119,133],[22,37],[10,119],[56,70],[81,101]\n",
    "                  ,[119,34],[22,56],[103,110],[56,90],[81,90],[119,89],[22,45],[102,119]],dtype=\"float32\")\n",
    "inputs=torch.from_numpy(inputs)\n",
    "targets=torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here I am using 15 training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here TensorDataset is created which allows acces to rows from inputs and targets as tuples and provides standard APIs for working with many different\n",
    "types of datasets in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 73.,  67.,  43.],\n",
       "         [ 91.,  88.,  64.],\n",
       "         [ 87., 134.,  58.]]),\n",
       " tensor([[ 56.,  70.],\n",
       "         [ 81., 101.],\n",
       "         [119., 133.]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds=TensorDataset(inputs,targets)\n",
    "train_ds[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now I will create a DataLoader which can split the data into batches of predefined size while training.\n",
    "\n",
    "- We are using batching because if we are loading 1000 of data to memory it will create computational challenges to system, so to avoid it we can use batches and send to memory for computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Data Loader\n",
    "batch_size=5\n",
    "train_dl= DataLoader(train_ds,batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data loader is typically used in a for-in loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches:\n",
      "tensor([[102.,  43.,  37.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 73.,  67.,  43.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [ 73.,  67.,  43.]])\n",
      "tensor([[ 22,  56],\n",
      "        [ 81,  90],\n",
      "        [ 56,  70],\n",
      "        [119, 133],\n",
      "        [ 56,  90]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for xb,yb in train_dl:\n",
    "    print(\"Batches:\")\n",
    "    print(xb)\n",
    "    print(yb)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since shuffle is set to True, it shuffles the training data before creating batches.\n",
    "- Shuffle helps randomize the input to the optimization algorithm, which can lead to faster reduction in loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### nn.Linear\n",
    "\n",
    "- Instead of intializing the weights and biases manually, we can define the model using teh nn.Linear whcih does int automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3925, -0.0574,  0.3230],\n",
      "        [-0.5157, -0.1823, -0.3197]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1249, 0.3237], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## Define model\n",
    "model=nn.Linear(3,2) # 3 is the number of inputs and 2 is the number of outputs\n",
    "print(model.weight) # Model itself creates weights \n",
    "print(model.bias)   # Model itself creates biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch model have a helpful parameter called .parameters method, which returns  a list containing all the weights and matrices present in the model.\n",
    "\n",
    "- For Linear regression model, we have one weight matrix and one bias matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[-0.3925, -0.0574,  0.3230],\n",
       "         [-0.5157, -0.1823, -0.3197]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1249, 0.3237], requires_grad=True)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "\n",
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-18.4871, -63.2803],\n",
      "        [-19.9750, -83.1037],\n",
      "        [-22.9839, -87.5067],\n",
      "        [-30.4307, -71.9432],\n",
      "        [ -9.8606, -75.1346],\n",
      "        [-18.4871, -63.2803],\n",
      "        [-19.9750, -83.1037],\n",
      "        [-22.9839, -87.5067],\n",
      "        [-30.4307, -71.9432],\n",
      "        [ -9.8606, -75.1346],\n",
      "        [-18.4871, -63.2803],\n",
      "        [-19.9750, -83.1037],\n",
      "        [-22.9839, -87.5067],\n",
      "        [-30.4307, -71.9432],\n",
      "        [ -9.8606, -75.1346]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Generating Predictions\n",
    "preds=model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "- Instead of defining a loss function manually, we can use built-in loss function mse_loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## nn.functional has lots of loss functions,error functions and several other utilitties.\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can use mse_loss to find the measn squared error loss\n",
    "loss_fn=F.mse_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(411.3709, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss=loss_fn(model(inputs),targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "- Instead of manually manipualting the model's weights and biases using gradients, we can use the optimizer optim.SGD. SGD stands for stochastic gradient descent. It is alled stochastic becasue samples are selected in batches(often with random shuffling) instead of a single group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Optimizer\n",
    "\n",
    "opt=torch.optim.SGD(model.parameters(),lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that model.parameters() is passed as an argument to optim.SGD, so that optimizer knows which matrices should be modified during the update step. Also, we can specify a learning rate which controls the amount by which parameters\n",
    "are modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility function to train the model.\n",
    "def fit(num_epochs,model,loss_fn,opt,train_dl):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in train_dl:\n",
    "            pred=model(xb)\n",
    "            loss=loss_fn(pred,yb)\n",
    "            loss.backward() #compute gradients\n",
    "            opt.step() #updating parameters using gradients\n",
    "            opt.zero_grad() #reseting the gradient to zero\n",
    "            \n",
    "            #printing the progress\n",
    "            \n",
    "            if (epoch+1)%10==0:\n",
    "                print(\"Epoch [{}/{}],Loss {:.4f}\".format(epoch+1,num_epochs,loss.item() ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500],Loss 407.5816\n",
      "Epoch [10/500],Loss 758.2211\n",
      "Epoch [10/500],Loss 124.6696\n",
      "Epoch [20/500],Loss 512.7873\n",
      "Epoch [20/500],Loss 191.4151\n",
      "Epoch [20/500],Loss 549.3286\n",
      "Epoch [30/500],Loss 81.0687\n",
      "Epoch [30/500],Loss 880.6526\n",
      "Epoch [30/500],Loss 328.6092\n",
      "Epoch [40/500],Loss 289.9973\n",
      "Epoch [40/500],Loss 904.0948\n",
      "Epoch [40/500],Loss 91.7841\n",
      "Epoch [50/500],Loss 106.1917\n",
      "Epoch [50/500],Loss 561.4618\n",
      "Epoch [50/500],Loss 573.0817\n",
      "Epoch [60/500],Loss 267.1618\n",
      "Epoch [60/500],Loss 427.7354\n",
      "Epoch [60/500],Loss 608.8589\n",
      "Epoch [70/500],Loss 145.0195\n",
      "Epoch [70/500],Loss 85.4476\n",
      "Epoch [70/500],Loss 1027.8165\n",
      "Epoch [80/500],Loss 708.2224\n",
      "Epoch [80/500],Loss 437.5897\n",
      "Epoch [80/500],Loss 147.6322\n",
      "Epoch [90/500],Loss 499.5178\n",
      "Epoch [90/500],Loss 449.1245\n",
      "Epoch [90/500],Loss 325.8586\n",
      "Epoch [100/500],Loss 147.2988\n",
      "Epoch [100/500],Loss 725.2679\n",
      "Epoch [100/500],Loss 422.7929\n",
      "Epoch [110/500],Loss 320.0811\n",
      "Epoch [110/500],Loss 365.5114\n",
      "Epoch [110/500],Loss 593.3062\n",
      "Epoch [120/500],Loss 375.7444\n",
      "Epoch [120/500],Loss 772.1268\n",
      "Epoch [120/500],Loss 152.4076\n",
      "Epoch [130/500],Loss 160.0983\n",
      "Epoch [130/500],Loss 512.0404\n",
      "Epoch [130/500],Loss 568.8566\n",
      "Epoch [140/500],Loss 526.5078\n",
      "Epoch [140/500],Loss 86.8338\n",
      "Epoch [140/500],Loss 634.8142\n",
      "Epoch [150/500],Loss 730.1051\n",
      "Epoch [150/500],Loss 151.0852\n",
      "Epoch [150/500],Loss 410.1485\n",
      "Epoch [160/500],Loss 547.0194\n",
      "Epoch [160/500],Loss 471.6057\n",
      "Epoch [160/500],Loss 262.1338\n",
      "Epoch [170/500],Loss 571.8934\n",
      "Epoch [170/500],Loss 400.7239\n",
      "Epoch [170/500],Loss 329.1009\n",
      "Epoch [180/500],Loss 561.1693\n",
      "Epoch [180/500],Loss 88.3591\n",
      "Epoch [180/500],Loss 594.2568\n",
      "Epoch [190/500],Loss 133.7909\n",
      "Epoch [190/500],Loss 836.8605\n",
      "Epoch [190/500],Loss 348.1927\n",
      "Epoch [200/500],Loss 413.0583\n",
      "Epoch [200/500],Loss 39.4746\n",
      "Epoch [200/500],Loss 842.9974\n",
      "Epoch [210/500],Loss 130.7812\n",
      "Epoch [210/500],Loss 751.4368\n",
      "Epoch [210/500],Loss 384.9757\n",
      "Epoch [220/500],Loss 695.0757\n",
      "Epoch [220/500],Loss 203.8375\n",
      "Epoch [220/500],Loss 404.5048\n",
      "Epoch [230/500],Loss 112.6163\n",
      "Epoch [230/500],Loss 782.5181\n",
      "Epoch [230/500],Loss 403.9850\n",
      "Epoch [240/500],Loss 365.9364\n",
      "Epoch [240/500],Loss 820.1783\n",
      "Epoch [240/500],Loss 90.3048\n",
      "Epoch [250/500],Loss 99.4118\n",
      "Epoch [250/500],Loss 412.8880\n",
      "Epoch [250/500],Loss 768.3311\n",
      "Epoch [260/500],Loss 48.3938\n",
      "Epoch [260/500],Loss 575.6453\n",
      "Epoch [260/500],Loss 618.8646\n",
      "Epoch [270/500],Loss 108.3970\n",
      "Epoch [270/500],Loss 795.7791\n",
      "Epoch [270/500],Loss 403.3867\n",
      "Epoch [280/500],Loss 79.5719\n",
      "Epoch [280/500],Loss 286.2154\n",
      "Epoch [280/500],Loss 910.7469\n",
      "Epoch [290/500],Loss 562.7913\n",
      "Epoch [290/500],Loss 157.9863\n",
      "Epoch [290/500],Loss 539.1884\n",
      "Epoch [300/500],Loss 762.3260\n",
      "Epoch [300/500],Loss 84.2687\n",
      "Epoch [300/500],Loss 416.3988\n",
      "Epoch [310/500],Loss 886.3407\n",
      "Epoch [310/500],Loss 59.2024\n",
      "Epoch [310/500],Loss 328.7597\n",
      "Epoch [320/500],Loss 366.8843\n",
      "Epoch [320/500],Loss 537.5557\n",
      "Epoch [320/500],Loss 373.2050\n",
      "Epoch [330/500],Loss 79.8586\n",
      "Epoch [330/500],Loss 419.8236\n",
      "Epoch [330/500],Loss 783.3436\n",
      "Epoch [340/500],Loss 564.1945\n",
      "Epoch [340/500],Loss 109.1996\n",
      "Epoch [340/500],Loss 575.8170\n",
      "Epoch [350/500],Loss 547.8715\n",
      "Epoch [350/500],Loss 136.3438\n",
      "Epoch [350/500],Loss 571.7765\n",
      "Epoch [360/500],Loss 164.0486\n",
      "Epoch [360/500],Loss 558.2204\n",
      "Epoch [360/500],Loss 533.0692\n",
      "Epoch [370/500],Loss 567.7105\n",
      "Epoch [370/500],Loss 316.5015\n",
      "Epoch [370/500],Loss 378.5012\n",
      "Epoch [380/500],Loss 322.5163\n",
      "Epoch [380/500],Loss 747.1293\n",
      "Epoch [380/500],Loss 237.4753\n",
      "Epoch [390/500],Loss 135.2622\n",
      "Epoch [390/500],Loss 358.8289\n",
      "Epoch [390/500],Loss 793.8641\n",
      "Epoch [400/500],Loss 158.7293\n",
      "Epoch [400/500],Loss 552.2082\n",
      "Epoch [400/500],Loss 535.3378\n",
      "Epoch [410/500],Loss 1002.2999\n",
      "Epoch [410/500],Loss 108.0859\n",
      "Epoch [410/500],Loss 147.7264\n",
      "Epoch [420/500],Loss 127.5327\n",
      "Epoch [420/500],Loss 1032.3831\n",
      "Epoch [420/500],Loss 105.6247\n",
      "Epoch [430/500],Loss 566.4351\n",
      "Epoch [430/500],Loss 419.6006\n",
      "Epoch [430/500],Loss 310.6357\n",
      "Epoch [440/500],Loss 61.9188\n",
      "Epoch [440/500],Loss 849.2650\n",
      "Epoch [440/500],Loss 373.2980\n",
      "Epoch [450/500],Loss 124.8704\n",
      "Epoch [450/500],Loss 873.9256\n",
      "Epoch [450/500],Loss 270.6970\n",
      "Epoch [460/500],Loss 596.7950\n",
      "Epoch [460/500],Loss 585.9075\n",
      "Epoch [460/500],Loss 81.8768\n",
      "Epoch [470/500],Loss 723.9984\n",
      "Epoch [470/500],Loss 104.0374\n",
      "Epoch [470/500],Loss 463.5185\n",
      "Epoch [480/500],Loss 537.9412\n",
      "Epoch [480/500],Loss 186.4429\n",
      "Epoch [480/500],Loss 531.6113\n",
      "Epoch [490/500],Loss 530.3751\n",
      "Epoch [490/500],Loss 357.5823\n",
      "Epoch [490/500],Loss 375.3130\n",
      "Epoch [500/500],Loss 580.2194\n",
      "Epoch [500/500],Loss 294.3571\n",
      "Epoch [500/500],Loss 401.4351\n"
     ]
    }
   ],
   "source": [
    "fit(500,model,loss_fn,opt,train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 54.5147,  65.1891],\n",
       "        [ 70.4950, 101.0628],\n",
       "        [120.6551,  87.5892],\n",
       "        [ 27.5420,  49.1075],\n",
       "        [ 79.8919, 116.6575],\n",
       "        [ 54.5147,  65.1891],\n",
       "        [ 70.4950, 101.0628],\n",
       "        [120.6551,  87.5892],\n",
       "        [ 27.5420,  49.1075],\n",
       "        [ 79.8919, 116.6575],\n",
       "        [ 54.5147,  65.1891],\n",
       "        [ 70.4950, 101.0628],\n",
       "        [120.6551,  87.5892],\n",
       "        [ 27.5420,  49.1075],\n",
       "        [ 79.8919, 116.6575]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [ 10., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119.,  34.],\n",
       "        [ 22.,  56.],\n",
       "        [103., 110.],\n",
       "        [ 56.,  90.],\n",
       "        [ 81.,  90.],\n",
       "        [119.,  89.],\n",
       "        [ 22.,  45.],\n",
       "        [102., 119.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jovian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "window.require && require([\"base/js/namespace\"],function(Jupyter){Jupyter.notebook.save_checkpoint()})"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[jovian] Attempting to save notebook..\n",
      "[jovian] Uploading notebook..\n",
      "[jovian] Capturing environment..\n",
      "[jovian] Committed successfully! https://jovian.ml/nithin212/pytorch-linear-regression-using-buitlins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://jovian.ml/nithin212/pytorch-linear-regression-using-buitlins'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jovian.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
