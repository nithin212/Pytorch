{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73.  67.  43.]\n",
      " [ 91.  88.  64.]\n",
      " [ 87. 134.  58.]\n",
      " [102.  43.  37.]\n",
      " [ 69.  96.  70.]]\n"
     ]
    }
   ],
   "source": [
    "# Input (temp,rainfall,humidity)\n",
    "inputs= np.array([[73,67,43],[91,88,64],[87,134,58],[102,43,37],[69,96,70]],dtype=\"float32\")\n",
    "print(inputs)\n",
    "#Dtype is in floating point because our model will be predicting in floating points not in integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 56.  70.]\n",
      " [ 81. 101.]\n",
      " [119. 133.]\n",
      " [ 22.  37.]\n",
      " [103. 119.]]\n"
     ]
    }
   ],
   "source": [
    "# Target Yield of apple and oranges\n",
    "\n",
    "targets=np.array([[56,70],[81,101],[119,133],[22,37],[103,119]],dtype=\"float32\")\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 73.,  67.,  43.],\n",
      "        [ 91.,  88.,  64.],\n",
      "        [ 87., 134.,  58.],\n",
      "        [102.,  43.,  37.],\n",
      "        [ 69.,  96.,  70.]])\n",
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "# Converting inputs and targets to tensors\n",
    "inputs=torch.from_numpy(inputs)\n",
    "targets=torch.from_numpy(targets)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2398, -0.3148,  0.0052],\n",
      "        [-2.2454,  0.2593, -0.2694]], requires_grad=True) \n",
      " tensor([-0.2600,  0.0953], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Weights and biases\n",
    "w=torch.randn(2,3,requires_grad=True)\n",
    "b=torch.randn(2,requires_grad=True)\n",
    "print(w,\"\\n\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is simply a function that performs a amtrix multiplication of the inptus\n",
    "and the weights w(transposed) and adds the bias b (replicated) for each observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x):\n",
    "    return x@w.t()+b\n",
    "# @ in pytorch does the matrix multiplication and .t does teh transpose of any matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating predictions with random weigh\n",
    "preds=model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -3.6164, -158.0243],\n",
       "        [  -5.7996, -198.6517],\n",
       "        [ -21.2697, -176.1247],\n",
       "        [  10.8620, -227.7480],\n",
       "        [ -13.5628, -148.7949]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### There is a huge differnce between actual values and the predicted values.This because we have intialised random values to the weights and biases.\n",
    "###### We will never get negative number of oranges and apples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -59.6164, -228.0243],\n",
       "        [ -86.7996, -299.6517],\n",
       "        [-140.2697, -309.1247],\n",
       "        [ -11.1380, -264.7480],\n",
       "        [-116.5628, -267.7949]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds-targets  # this is the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "- We first need a way to evaluate how well our model is performing later we will improve our model.\n",
    "- We will use Mean Squared Error(MSE) to check how well our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(t1,t2):\n",
    "    diff=t1-t2\n",
    "    return torch.sum(diff*diff)/diff.numel()\n",
    "\n",
    "# torch.sum will return the sum of squares of difference\n",
    "# .numel will return number of elements in a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=mse(preds,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(42362.4766, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by looking at the loss we can intrepret that on an average each element in the prediction difffers almost about 205(square root of loss :42362.476).The result is called loss, because it indicates how bad the model is at predicting the target variables. Lower the loss better the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To improve the model we will use a technique called gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can actually say that loss is a function of weights\n",
    "- That is the reason why I have set requires_grad as True for weights and biases.\n",
    "- One moe thing we need to keep in mind is inputs never changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradients are stored in the .grad property of respective tensors. Note that the derivate of the loss w.r.t the weights is itself a matrix, with the same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2398, -0.3148,  0.0052],\n",
      "        [-2.2454,  0.2593, -0.2694]], requires_grad=True)\n",
      "tensor([[ -6726.6265,  -8419.5527,  -4965.1646],\n",
      "        [-23258.0156, -24032.4336, -15090.6621]])\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Loss is a quadratic function of our weights and biases and our objective is to find the set of weights where the loss it lowest\n",
    "\n",
    "- If a gradient element is positive:\n",
    "  - Increasing the element's value slightly will increase the loss.\n",
    "  - Decreasing the elments's value slightly will decrease the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If gradient element is negative\n",
    " - Increasing the element's value slightly will decrease the loss.\n",
    " - Increasing the element's value slightly will increaset the loss.\n",
    " \n",
    "- Idea is that we always moves to the opposite of derivatives\n",
    "\n",
    "#### This forms the basis for the optimization algorithm that we'll use to improve our model.\n",
    "\n",
    "- Before we proceed, we reset the gradeints to zero by calling .zero_() method.\n",
    "- We need to do this because PyTorch accumulates, gradients i.e the next time we call .backward on the loss,the new grasdient values will get added to the existing gradient values, which may lead to unexpected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([0., 0.])\n"
     ]
    }
   ],
   "source": [
    "w.grad.zero_()\n",
    "b.grad.zero_()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Weights biases using gradient descent\n",
    "\n",
    "\n",
    "- Now here I will use optimization algorithm which forms the base of all training in deep learning.\n",
    "\n",
    "- The gradient descent optimization algorithm has the following steps.\n",
    "\n",
    "   1. Generate Predictions\n",
    "   2. Calculate the loss\n",
    "   3. Compute graident w.r.t the weights and biases\n",
    "   4. Adjust the weights by subtracting a small quantity proportional to the gradient.\n",
    "   5. Reset the gradient to zero\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  -3.6164, -158.0243],\n",
      "        [  -5.7996, -198.6517],\n",
      "        [ -21.2697, -176.1247],\n",
      "        [  10.8620, -227.7480],\n",
      "        [ -13.5628, -148.7949]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Generating prediction\n",
    "\n",
    "preds=model(inputs)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(42362.4766, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Caluclating loss\n",
    "loss=mse(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -6726.6265,  -8419.5527,  -4965.1646],\n",
      "        [-23258.0156, -24032.4336, -15090.6621]]) \n",
      " tensor([ -82.8773, -273.8687])\n"
     ]
    }
   ],
   "source": [
    "# Computing Gradients\n",
    "loss.backward()\n",
    "print(w.grad,\"\\n\",b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Finally we will update weights and biases using gradients computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    w-=w.grad * 1e-5\n",
    "    b-=b.grad * 1e-5\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- torch.no_grad is used to indicate to Pytorch that it shouldn't track, calculate or modify gradients while updating weights and biases\n",
    "- We multiply the gradient with a really small number(10^-5), to ensure that we don't modify the weights by a really large amount, since we only want a sa,ll step in downhill direction of the gradient. This number is called leraning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3071, -0.2306,  0.0549],\n",
      "        [-2.0128,  0.4997, -0.1185]], requires_grad=True) \n",
      " tensor([-0.2591,  0.0980], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(w,\"\\n\",b)   # New weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28822.2598, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Calculating loss with newer weights and biases.\n",
    "preds=model(inputs)\n",
    "loss=mse(preds,targets)\n",
    "print(loss)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Before loss was 42362.4766 and now it has reduced to 28822.2598\n",
    "- Still we need to adjsut bias and weight to get good loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for multiple epochs\n",
    "\n",
    "To reduce the loss further, we can repeat the process of adjusting the weights and biases using the gradients multiple times. Each iteration is called an epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 100 epochs\n",
    "for i in range(1000):\n",
    "    preds=model(inputs)\n",
    "    loss=mse(preds,targets)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w-=w.grad*1e-5\n",
    "        b-=b.grad*1e-5\n",
    "        w.grad.zero_()\n",
    "        b.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4607, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "preds=model(inputs)\n",
    "loss=mse(preds,targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 57.2304,  70.4539],\n",
      "        [ 81.4034,  99.5389],\n",
      "        [120.4147, 135.2989],\n",
      "        [ 21.6300,  37.6197],\n",
      "        [100.1509, 116.8400]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [119., 133.],\n",
      "        [ 22.,  37.],\n",
      "        [103., 119.]])\n"
     ]
    }
   ],
   "source": [
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
